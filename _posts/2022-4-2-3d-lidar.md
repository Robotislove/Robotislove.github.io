---
layout: post
title: 大场景三维点云的语义分割综述学习笔记
date: 2022-04-02
author: lau
tags: [SLAM, Note]
comments: true
toc: true
pinned: false
---

大场景三维点云语义分割综述笔记。

<!-- more -->

## 一．点云语义分割

输入原始点云（x，y，z，intensity），得到每个三维点的语义类别。如图所示，不同颜色代表不同类别。

![](https://ask.qcloudimg.com/http-save/yehe-5926470/1tqvdg3jqe.jpeg?imageView2/2/w/1620)

![](https://ask.qcloudimg.com/http-save/yehe-5926470/yt838i5gfr.jpeg?imageView2/2/w/1620)

### 面临挑战

- 点云的无序性：点云的输入是无序的，点云顺序的变化不应影响结果。目前PoinNet等基于点的位置及k近邻编码的方法能够解决这个问题。
- 点云旋转不变性表达：对于点云的SO3变换，应当不影响点云的语义类别，但现有方法不具有旋转平移不变性等。
- 点云特征的有效提取：传统手工特征效果不足，CNN不能直接应用与点云数据，如何有效捕捉点云之间的位置关系及特征关系仍是一个开放的问题。
- 大场景点云处理：对于大场景点云，点云数量及分布范围较大，这种场景通常是由多个物体组成的，不能像对待单个物体一样处理。如何优雅地通用地实时地高效地处理大场景点云，仍是未解决的问题。

## Methods

主要关注最新的应用在无人驾驶场景的大场景点云语义分割方法。这些方法大致分为两类，基于2D的和基于3D的。2D的基本思路是将点云按照某种方式投影到2D平面，并应用成熟的2D语义分割网络做处理。3D的方法一般是直接在3D空间提取特征信息。还有一些方法会融合图像的信息来帮助点云的语义分割。

###  基于2D

![](https://ask.qcloudimg.com/http-save/yehe-5926470/6nv1tdi4u5.png?imageView2/2/w/1620)

具有代表性的工作是：

SqueezeSeg (https://github.com/BichenWuUCB/SqueezeSeg)和 SqueezeSegV2 (https://github.com/xuanyuzhou98/SqueezeSegV2）分别发表在机器人顶会ICRA2018和ICRA2019上。后来的方法基本上按照类似SqueezeSeg将点云投影并用2D网络处理。

题目:VIASEG: VISUAL INFORMATION ASSISTED LIGHTWEIGHT POINT CLOUD SEGMENTATION

论文: https://ieeexplore.ieee.org/abstract/document/8803061/2019 IEEE International Conference on Image Processing (ICIP)

简介：图像的色彩信息能够提供丰富的视觉信息，作者将颜色信息以数据级别(data-level)嵌入到点云中，可以提升点云语义分割的表现。一个基于Super Squeeze Residual module和 Semantic Connection的多尺度的全卷积网络VIASeg被提出。在提高整体表现的同时保证了实时性。

主要贡献：

1.一个基于数据级别融合的视觉信息辅助点云的语义分割方法被提出，这样消除了对于预训练和后处理的依赖。

2.在SSR模块，SC和atrous separable convolution的帮助下，一个针对彩色点云的更深但是轻量级的网络VIASeg被提出，并且获得了state-of-the-art的表现。

3.随着传感器技术的发展，未来传感器将会可以直接提供带有色彩的点云。我们相信我们提出的方法可以对于多模态（视觉+点云）的语义分割产生持续贡献。

![](https://ask.qcloudimg.com/http-save/yehe-5926470/nqs14ebc7z.jpeg?imageView2/2/w/1620)
![](https://ask.qcloudimg.com/http-save/yehe-5926470/9zvif2qgyv.jpeg?imageView2/2/w/1620)

方法：

1.数据预处理：根据激光雷达和相机的标定外参，将图像和点云对其，这样每个3D点将会拥有色彩信息。将点云投影到前视图（Front-View），每个2D点保留8维特征（x,y,z,intensity,depth,r,g,b）。网络的输入图像大小为64x512x8。

2.Super Squeeze Residual Module （SSR）: 我们基于Fire module和ResNet提出这个模块如图2所示，实验证明有提升。

3.网络架构：如图2所示，Encoder-decoder的结构，共有4个尺度的encoder。并且encoder和decoder之间的链接被高维的语义信息净化，叫做Semantic Connection（SC）。
-  Atrous Separable Convolution: 3D->2D的映射会有像素丢失，这对于小目标物体影响严重，在下采样中如何保留有价值信息是至关重要的。在SqueezeSeg中采用的小内核池化无法处理这种情况。SqueezeSegV2 使用了一个大的最大池化，它对丢失的数据不那么敏感，但是它也丢失了更多的信息。由于atrous convolution对此不敏感，可以调整感受野，并且可以降低计算量，因此我们替换池化（pooling）为atrous separable convolution 来降采样特征图（feature map）。
-  Semantic Connection：低维度（low-scale）的特征噪声大，不能够提供充足信息，直接encoder-decder的skip-connection会带来噪声，因此需要更干净的特征。高维度（high-scale）的特征更纯净，因此用邻近的encoder的高维度特征上采样之后与decoder相连。

实验：使用KITTI的3D object数据集，和SOTA方法做了对比，可以看出效果有一定提升，但并不明显。

![](https://ask.qcloudimg.com/http-save/yehe-5926470/zbliektbad.png?imageView2/2/w/1620)

题目：LU-Net: An Efficient Network for 3D LiDAR Point Cloud Semantic Segmentation Based on End-to-End-Learned 3D Features and U-Net

论文：https://arxiv.org/abs/1908.11656

arxiv 2019.08.30

简介：提出了LU-Net (for LiDAR U-Net)，首先通过每个点及其3D邻域点集提取高层次3D特征，然后这些特征被投影到2D多通道前视图（range-image）。使用U-Net做分割，保证了精度和速度。

主要贡献：LU-Net受益于高层次3D特征的提取，并将3D局部特征嵌入到2D图中，可以被U-Net分割网络高效地利用。我们的方法超过了基于2D图（range-image）的state-of-the-art方法。

方法：首先获得每个点的近邻点集如图4，并经过多层感知机（MLP）得到N个点的3D局部特征如图5，将每个点投影到2D前视图上（range-image）如图3，将3D特征保留，可以得到一个2D图HxWxN(H高，W宽，N为3D特征的维度也为此时2D图的通道数)。将构建的2D图送入U-Net，如图6，最终得到每个点的语义类别标签。

实验：这个工作虽然改动不多，加了3D局部特征到U-Net中，但是提升的效果还是很明显的。

![](https://ask.qcloudimg.com/http-save/yehe-5926470/hu7tcfyaxp.jpeg?imageView2/2/w/1620)

![](https://ask.qcloudimg.com/http-save/yehe-5926470/ktvl9c045p.png?imageView2/2/w/1620)

![](https://ask.qcloudimg.com/http-save/yehe-5926470/hjahjjpy8r.png?imageView2/2/w/1620)

![](https://ask.qcloudimg.com/http-save/yehe-5926470/uwfsizmm54.png?imageView2/2/w/1620)

![](https://ask.qcloudimg.com/http-save/yehe-5926470/eca6efu39k.png?imageView2/2/w/1620)

题目：RangeNet++: Fast and Accurate LiDAR Semantic Segmentation

论文：www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/milioto2019iros.pdf

2019 IEEE/RSJ International Conference on Intelligent Robots and Systems （IROS）

代码：https://github.com/PRBonn/rangenet_lib

      https://github.com/PRBonn/lidar-bonnetal

简介：在这篇论文中，我们将目前仅针对lidar的语义分割技术向前推进，以便为车辆提供另一个独立的语义信息来源。该方法能够准确地对激光雷达点云进行全语义分割。我们利用距离图像作为中间表示，并结合利用旋转激光雷达传感器模型的卷积神经网络(CNN)。为了获得准确的结果，我们提出了一种新的后处理算法来处理这种中间表示产生的问题，如离散化误差和模糊的CNN输出。我们实现并彻底评估了我们的方法，包括与当前技术状态的几次比较。我们的实验表明，我们的方法优于最先进的方法，同时仍然在一个嵌入式GPU上运行。

主要贡献：1. 准确的点云语义分割，超过其他方法。2.得到原始点云的所有语义标签，比变了点的损失。3.能够工作在嵌入式电脑上达到实时，适合应用在机器人和移动平台上。

方法：如图2

1.3D->2D：投影到前视图上。

2.使用DarkNet53作为2D网络的backbone，如图3.

3.后处理：2D分割后的结果重投影回3D空间会有拖影，作者采用GPU的kNN得到每个点的语义投票得分（认为相邻点的语义类别应当相同）。

实验：在SemanticKITTI上做实验，由于这个数据集是他们组做的，并且当时还没有发布，因此没有其他方法在这个数据集上测过，其他方法都是作者在SemanticKITTI上自己复现的，算是在这个数据集上的第一个工作叭。
![](https://ask.qcloudimg.com/http-save/yehe-5926470/bibbncahss.jpeg?imageView2/2/w/1620)

![](https://ask.qcloudimg.com/http-save/yehe-5926470/vicwe7oey9.jpeg?imageView2/2/w/1620)
![](https://ask.qcloudimg.com/http-save/yehe-5926470/e3qdhp1lpi.jpeg?imageView2/2/w/1620)

### 基于3D

题目：RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds

论文：https://arxiv.org/abs/1911.11236

arxiv 2019.11.25

代码：https://github.com/QingyongHu/RandLA-Net            （No code yet）

简介：提出了RandLA-Net，一个高效轻量的网络可以在大场景点云中直接得到逐点的语义信息。采用随机采样点云而不是复杂的采样策略，为了克服随机采样可能带来的关键特征丢失，一个新颖的局部特征聚合模块被提出。我们的方法可以一次推理（inference）处理一百万个点，并且比现有方法快200x倍。在Semantic3D和SemanticKITTI这两个大场景数据集上做了测试。

主要贡献：1.我们分析并比较了现有的采样策略，发现随机采样（random sampling）是对于大场景点云学习最合适的策略。2.我们提出了高效的局部特征聚合模块可以通过不断提高每个点的感受野自动地保留复杂的局部结构特征。3.我们证明了显著的内存和计算收益超过基线方法（baselines），并超过了最先进的语义分割方法在多个大场景点云数据集上。

方法：

1. 首先说明了随机采样对于大场景点云的采样是很合适的，对比了几个采样策略，只有随机采样是满足实时性要求的。我个人认为这个地方比较的不合理，因为其他方法使用其他采样策略的前处理一般是将点云进行随机采样，在网络的中间层使用其他采样策略，这样既能保证实时性又能提高采样的有效性。例如，PointNet++中将输入点云先随机降采样到N个点（4096），再在网络的每一步encode的过程中使用FPS（farthest point sampling）策略。然而作者这里直接在10^6大小的点云上比较FPS和随机采样，在这种情况下FPS要耗时200秒，这是不够合理且没太大意义的。

2. Local Feature Aggregation：

       2.1 Local Spatial Encoding：一般的操作，先用kNN找每个点的K个最近邻点，得到相对位置关系的表达，如公式1。在把相对位置的特征和点集本身的特征concatenate起来组成这个点的特征，如图3绿色区域所示。

       2.2 Attentive Pooling：大部分现有的方法都是用max/average pooling来做特征的降采样，作者认为基于attention的pooling更好，于是对每个点的特征施加了一个可学习的共享参数的W，得到权重分数（公式2）并与特征相乘。最后将这K（近邻）个特征加在一起（公式3），如图3橘色区域所示。

       2.3 Dilated Residual Block：类似ResNet，作者一共用了两层特征聚集模块并做了一个Skip Connection，如图3蓝色区域所示。

实验：实验部分只看了在无人驾驶场景的SemanticKITTI数据集，效果有一定的提高但是有限，benchmark排行榜网（https://competitions.codalab.org/competitions/20331#results），目前最高的mIoU是60.0%。

![](https://ask.qcloudimg.com/http-save/yehe-5926470/ham0k95nml.jpeg?imageView2/2/w/1620)
![](https://ask.qcloudimg.com/http-save/yehe-5926470/z0vxh14s62.png?imageView2/2/w/1620)
![](https://ask.qcloudimg.com/http-save/yehe-5926470/053d7yxh4p.jpeg?imageView2/2/w/1620)
![](https://ask.qcloudimg.com/http-save/yehe-5926470/ixcezzsfut.jpeg?imageView2/2/w/1620)

题目：PASS3D: Precise and Accelerated Semantic Segmentation for 3D Point Cloud

论文：https://arxiv.org/abs/1909.01643

2019 IEEE/RSJ International Conference on Intelligent Robots and Systems （IROS）

简介：我们提出了一个两阶段的点云语义分割框架，首先对原始点云去除地面，快速聚类并优化候选点云簇，然后将点云簇进行坐标变化及数据增强，并训练一个点云分割网络得到每个点精确的语义标签。我们一阶段的方法可以在很短时间得到高质量的候选点云簇（proposals），大大降低后续的点云处理量，提高了整体速度。

主要贡献：1. 提出一个灵活的两阶段3D点云语义分割框架，结合了传统分割聚类算法和基于深度学习的优势，可以直接在3D空间获得特征。2.我们的一阶段可以得到精炼的高召回率的候选点云簇，极大降低了后续点云处理的计算量及耗时。3.我们的数据增广方法可以消除坐标偏差提高表现。4.在KITTI上测试优于SOTA。

方法：

整体流程如图2所示。

1.Stage-1：Accelerated cluster proposal

       1.1 Ground plane fitting：基于两个假设，地面点的分布符合平面，地面点的位置较低。

       1.2 Ring-based clustering：激光雷达用ring的信息，根据这个可以设定阈值快速欧式聚类，得到每个点所属的点云簇。

       1.3 Proposals refinement：对候选点云簇进行优化，如扩大bbox融入更多点，有些点由于距离地面较近被归入了地面，因此这里把它们找回来。最终的候选点云簇如图3所示，相同颜色代表统一簇，不同颜色代表不同簇。

2.Stage-2: Point-wise semantic segmentation

       2.1Data preparation：转换坐标系如图4所示，网络更易收敛。数据增广如图5所示，由于数据的分布（如汽车的朝向等）会带来坐标偏差，因此对每个点云簇进行旋转和翻转变换操作，使得整体的分布是均匀的。

       2.2Learning-based semantic segmentation：使用PointNet++作为backbone（有更强的可以直接替换），输入网络每个点的特征为（x,y,z,intensity,n），n是相对点数。

实验：在KITTI上测试，优于SOTA。一阶段仅仅耗时5ms提出30个候选簇可以得到89.5%的逐点召回率，将原始点云近30k个点降低到5k个点，降低了后续的计算量。

![](https://ask.qcloudimg.com/http-save/yehe-5926470/yiq7fmc0l4.jpeg?imageView2/2/w/1620)

![](https://ask.qcloudimg.com/http-save/yehe-5926470/1qvx6jady6.png?imageView2/2/w/1620)

![](https://ask.qcloudimg.com/http-save/yehe-5926470/ekovv0uaft.png?imageView2/2/w/1620)
![](https://ask.qcloudimg.com/http-save/yehe-5926470/0393cukfqh.png?imageView2/2/w/1620)
![](https://ask.qcloudimg.com/http-save/yehe-5926470/wfrynybdnq.png?imageView2/2/w/1620)

题目：Ground-Aware Point Cloud Semantic Segmentation for Autonomous Driving

论文：https://jianbojiao.com/pdfs/ACMMM.pdf

2019 27th ACM International Conference on Multimedia （ACMMM）

代码：https://github.com/Jaiy/Ground-aware-Seg          （No code yet）

简介：我们提出了意识到地面（ground-aware）的框架来缓解无人驾驶场景激光雷达点云所带来的稀疏性问题。首先分割地面，并使用弱监督隐式地建模地面信息，用新的ground-aware attention module来获取意识到地面的特征，这个模块可以捕获地面和物体的长期依赖，有助于只有少量点的小物体的语义分割表现。

主要贡献：

1.我们提出了一个ground-aware attention网络来对无人驾驶场景的稀疏激光点云做语义分割。

2.我们提出了一个ground-aware attention module来有效的建模地面和物体的长期依赖关系。

3.实验表明我们的方法优于SOTA。

方法：

1.地面粗分割：将地面分为若干段，每段用RANSAC方法拟合一个平面，最终组成地面。将整个场景分为地面点云和物体点云两部分。

2.Region Feature Extraction：受到图表达的启发，根据原始的几何关系将点云分成若干超点（superpoints）来减少整个点云的规模。在两个点云中都进行graph-based partition（具体算法那没有细讲）。之后对每个点簇（group of points）使用PointNet提取特征。每个点得到一个64维特征，每个group得到一个512维的superpoint特征，拼接在一起得到576维特征。如图2蓝色框所示。

3.Ground-Aware Attention Module：注意力机制可以建模远距离的区域相关关系，我们将其拓展到3D点云中，据我们所知，这是第一次使用交叉注意力方式（cross-attention manner）将注意力机制用在三维点云语义分割中。

       3.1 Hard Attention：如图4所示，分别对Nx3（x,y,z）和 Nx4（x,y,z,d_g）的特征输入做特征提取，d_g是点和地面的距离。用attention block来建模这两组特征。

       3.2 Soft Attention：点到地面的距离不足以捕捉地面和物体之间的关系，因此将地面点云和物体点云分别提取特征得到g和f。attention的具体操作如公式2-4所示。最终得到N_ox512的特征。

同2中的576维特征拼接在一起送入MLP最终得到K个类别的概率。

4.Ground-Aware Loss Function：样本分布不均衡是这个任务共同的问题（人，车的类别远远小于背景类别），因此提出类别平衡的交叉熵损失loss，如公式5所示。

实验：用了阿里巴巴2018BDCI竞赛的数据集（80000帧）和一个没有开源的公司数据集（3000帧），没有用KITTI的数据集测试。其他方法在这些数据集上的表现应当是作者复现得到的。
![](https://ask.qcloudimg.com/http-save/yehe-5926470/tncvcvi40m.jpeg?imageView2/2/w/1620)
![](https://ask.qcloudimg.com/http-save/yehe-5926470/fmtll70b0g.jpeg?imageView2/2/w/1620)
![](https://ask.qcloudimg.com/http-save/yehe-5926470/ghqpl89uvg.png?imageView2/2/w/1620)
![](https://ask.qcloudimg.com/http-save/yehe-5926470/bkxer52b3l.png?imageView2/2/w/1620)
![](https://ask.qcloudimg.com/http-save/yehe-5926470/3susk4wm6n.png?imageView2/2/w/1620)
![](https://ask.qcloudimg.com/http-save/yehe-5926470/uz6h5des4p.png?imageView2/2/w/1620)
![](https://ask.qcloudimg.com/http-save/yehe-5926470/5e9hilrexq.jpeg?imageView2/2/w/1620)


